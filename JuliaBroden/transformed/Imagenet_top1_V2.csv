Entity,Year,Top1_accuracy,Type,Source
Alexnet,2012,0.633,Without Extra Training Data,ImageNet Classification with Deep Convolutional Neural Networks
Five base + five hires,2013,0.663,Without Extra Training Data,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
Vgg-19,2014,0.745,Without Extra Training Data,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
Inception v3,2015,0.788,Without Extra Training Data,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
Resnext-101 64x4 + multi-scale dense testing,2016,0.823,Without Extra Training Data,Billion-scale semi-supervised learning for image classification
Pnasnet-5,2017,0.829,Without Extra Training Data,Dual Path Networks
Gpipe,2018,0.844,Without Extra Training Data,Densely Connected Convolutional Networks
Advprop (efficientnet-b8),2019,0.855,Without Extra Training Data,Adversarial Examples Improve Image Recognition
Fix-efficientnet-b8 (maxup + cutmix),2020,0.858,Without Extra Training Data,Fixing the train-test resolution discrepancy: FixEfficientNet
"Mae (vit-h, 448)",2021,0.878,Without Extra Training Data,Masked Autoencoders Are Scalable Vision Learners
Alexnet - 7cnns + imagenet 2011 pretrain,2012,0.633,With Extra Training Data,Self-training with Noisy Student improves ImageNet classification
Vgg-19,2014,0.745,With Extra Training Data,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
Inception v3,2015,0.788,With Extra Training Data,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
Jft-300m finetuning,2017,0.792,With Extra Training Data,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
Resnext-101 32x48d,2018,0.854,With Extra Training Data,Xception: Deep Learning with Depthwise Separable Convolutions
Bit-l (resnet),2019,0.8754,With Extra Training Data,Big Transfer (BiT): General Visual Representation Learning
Efficientnet-l2-475 (sam),2020,0.8861,With Extra Training Data,Sharpness-Aware Minimization for Efficiently Improving Generalization
Coatnet-7,2021,0.9088,With Extra Training Data,CoAtNet: Marrying Convolution and Attention for All Data Sizes
