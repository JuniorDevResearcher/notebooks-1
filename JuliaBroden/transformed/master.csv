Entity,Year,Type,Source,Top1_accuracy,Top5_accuracy,Number_patent_filings
Advprop (efficientnet-b8),2019,Without Extra Training Data,Adversarial Examples Improve Image Recognition,0.855,0.973,
Alexnet,2012,Without Extra Training Data,ImageNet Classification with Deep Convolutional Neural Networks,0.633,0.846,
Alexnet - 7cnns + imagenet 2011 pretrain,2012,With Extra Training Data,Self-training with Noisy Student improves ImageNet classification,0.633,0.846,
Bit-l (resnet),2019,With Extra Training Data,Big Transfer (BiT): General Visual Representation Learning,0.8754,0.9846,
Coatnet-7,2021,With Extra Training Data,CoAtNet: Marrying Convolution and Attention for All Data Sizes,0.9088,,
Efficientnet-l2-475 (sam),2020,With Extra Training Data,Sharpness-Aware Minimization for Efficiently Improving Generalization,0.8861,,
Five base + five hires,2013,Without Extra Training Data,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,0.663,,
Fix-efficientnet-b8 (maxup + cutmix),2020,Without Extra Training Data,Fixing the train-test resolution discrepancy: FixEfficientNet,0.858,,
Fixefficientnet-b8,2020,Without Extra Training Data,Fixing the train-test resolution discrepancy: FixEfficientNet,,0.976,
Florence-coswim-h,2021,With Extra Training Data,Florence: A New Foundation Model for Computer Vision,,0.9902,
Gpipe,2018,Without Extra Training Data,Densely Connected Convolutional Networks,0.844,,
"Inception resnet v2 + multi-crop, multi-scale",2016,Without Extra Training Data,Squeeze-and-Excitation Networks,,0.963,
Inception v3,2015,With Extra Training Data,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,0.788,,
Inception v3,2015,Without Extra Training Data,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,0.788,0.944,
Jft-300m finetuning,2017,With Extra Training Data,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,0.792,0.947,
"Mae (vit-h, 448)",2021,Without Extra Training Data,Masked Autoencoders Are Scalable Vision Learners,0.878,,
Nasnet-a(6),2017,Without Extra Training Data,Learning Transferable Architectures for Scalable Image Recognition,,0.962,
Nfnet-f6 w/ sam,2021,Without Extra Training Data,High-Performance Large-Scale Image Recognition Without Normalization,,0.979,
Noisystudent (efficientnet-l2),2020,With Extra Training Data,Self-training with Noisy Student improves ImageNet classification,,0.987,
Overfeat - 7 accurate models,2013,Without Extra Training Data,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,,0.868,
Pnasnet-5,2017,Without Extra Training Data,Dual Path Networks,0.829,,
Resnet-101,2015,With Extra Training Data,Deep Residual Learning for Image Recognition,,0.9395,
Resnext-101 32x16d,2018,Without Extra Training Data,Exploring the Limits of Weakly Supervised Pretraining,,0.972,
Resnext-101 32x48d,2018,With Extra Training Data,Xception: Deep Learning with Depthwise Separable Convolutions,0.854,0.976,
Resnext-101 64x4 + multi-scale dense testing,2016,Without Extra Training Data,Billion-scale semi-supervised learning for image classification,0.823,,
Vgg-19,2014,With Extra Training Data,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,0.745,,
Vgg-19,2014,Without Extra Training Data,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,0.745,0.92,
World,2010,,,,,2560
World,2011,,,,,2577
World,2012,,,,,2949
World,2013,,,,,3175
World,2014,,,,,3676
World,2015,,,,,4617
World,2016,,,,,6077
World,2017,,,,,10216
World,2018,,,,,18282
World,2019,,,,,39840
World,2020,,,,,81472
World,2021,,,,,141241
