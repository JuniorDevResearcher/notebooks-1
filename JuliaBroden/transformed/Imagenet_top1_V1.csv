Entity,Year,Top1_accuracy,Method,Source
Without extra training data,2012,0.633,AlexNet,ImageNet Classification with Deep Convolutional Neural Networks
Without extra training data,2013,0.663,Five Base + Five HiRes,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
Without extra training data,2014,0.745,VGG-19,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
Without extra training data,2015,0.788,Inception V3,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
Without extra training data,2016,0.823,ResNeXt-101 64x4 + multi-scale dense testing,Billion-scale semi-supervised learning for image classification
Without extra training data,2017,0.829,PNASNet-5,Dual Path Networks
Without extra training data,2018,0.844,GPIPE,Densely Connected Convolutional Networks
Without extra training data,2019,0.855,AdvProp (EfficientNet-B8),Adversarial Examples Improve Image Recognition
Without extra training data,2020,0.858,Fix-EfficientNet-B8 (MaxUp + CutMix),Fixing the train-test resolution discrepancy: FixEfficientNet
Without extra training data,2021,0.878,"MAE (ViT-H, 448)",Masked Autoencoders Are Scalable Vision Learners
With extra training data,2012,0.633,AlexNet - 7CNNs + ImageNet 2011 pretrain,Self-training with Noisy Student improves ImageNet classification
With extra training data,2014,0.745,VGG-19,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
With extra training data,2015,0.788,Inception V3,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
With extra training data,2017,0.792,JFT-300M Finetuning,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
With extra training data,2018,0.854,ResNeXt-101 32x48d,Xception: Deep Learning with Depthwise Separable Convolutions
With extra training data,2019,0.8754,BiT-L (ResNet),Big Transfer (BiT): General Visual Representation Learning
With extra training data,2020,0.8861,EfficientNet-L2-475 (SAM),Sharpness-Aware Minimization for Efficiently Improving Generalization
With extra training data,2021,0.9088,CoAtNet-7,CoAtNet: Marrying Convolution and Attention for All Data Sizes
